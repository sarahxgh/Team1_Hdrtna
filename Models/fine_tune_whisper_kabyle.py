:# -*- coding: utf-8 -*-
"""fine_tune_whisper_kabyle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OB0VCxwPUgW8kEt63rrsgrfXnsB8Ei-F

"""
#gpu_info = !nvidia-smi
#gpu_info = '\n'.join(gpu_info)
#if gpu_info.find('failed') >= 0:
#  print('Not connected to a GPU')
#else:
#  print(gpu_info)


#imports 
from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import load_dataset, Audio
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union
import evaluate
from datasets import Dataset, Audio,DatasetDict
import pandas as pd


# Loading the dataset
dataset_path = '/home/team1/dataset/hdrtna_tamazight'
tsv_file = f'{dataset_path}/train.tsv'
audio_folder = f'{dataset_path}/audio'

def load_audio(example):
    example['audio'] = f"{audio_folder}/{example['audio']}"
    return example


data = pd.read_csv(tsv_file, sep='\t')

# Convert the DataFrame to a Hugging Face dataset
dataset = Dataset.from_pandas(data)


dataset = dataset.map(load_audio)
dataset = dataset.cast_column("audio", Audio())
split_dataset = dataset.train_test_split(test_size=0.2, seed=42)

common_voice = DatasetDict({
    "train": split_dataset["train"],
    "test": split_dataset["test"]
})

print(common_voice)



# Define the new tokens for Tamazight (these are the ones used in the dataset, there are more)
new_tokens = ["Č", "Ḍ", "Ǧ", "Ḥ", "Ɣ", "Ṛ", "Ṣ", "Ṭ", "Ẓ", "Ɛ", "ɛ", "ẓ", "ṭ", "ṣ", "ṛ", "ɣ", "ḥ", "ǧ", "ḍ", "č"]


# Load the Whisper model and tokenizer
model_name = "openai/whisper-medium"
feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)
tokenizer = WhisperTokenizer.from_pretrained(model_name, language="fr", task="transcribe")


# Add new tokens to the tokenizer
tokenizer.add_tokens(new_tokens)

# Load the processor
processor = WhisperProcessor.from_pretrained(model_name, language="fr", task="transcribe")

common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))

def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]
    batch["labels"] = tokenizer(batch["transcription"]).input_ids
    return batch


common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names['train'], num_proc=2)

# Data collator class
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

# Initialize data collator
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# Load evaluation metric
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = tokenizer.pad_token_id
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

model = WhisperForConditionalGeneration.from_pretrained(model_name)
model.generation_config.language = "fr"
model.resize_token_embeddings(len(tokenizer))

training_args = Seq2SeqTrainingArguments(
    output_dir="./FineTune_Whispermodium_ar",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
)

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)

# Start training
trainer.train()

